always_save_checkpoint = false
log_interval = 1
eval_only = false
device = "cpu"
compile = false
learning_rate = 100e-5
min_lr = 6e-5
decay_lr = false
lr_decay_iters = 600000
weight_decay = 1e-1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0
eval_interval = 50
eval_iters = 50
batch_size = 1
gradient_accumulation_steps = 64
warmup_iters = 2000
max_iters = 20000
backend = "nccl"
dtype = "float16"
epoch = 30
wandb_log = true
model_from = "__init__"
use_word_embedding = false
vocab_size = 22
block_size = 13
n_layer = 2
n_memory_layer = 2
n_head = 4
n_vocab = 22
d = 64
n_embd=64
sequence_length=13
dropout = 0.02
bias = false
freeze = false
